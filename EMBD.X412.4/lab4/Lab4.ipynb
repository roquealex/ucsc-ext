{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explore MapReduce: Copy and paste these lines to see how to use Map and Reduce functions in Spark. Copy and paste the session output to submit for this Lab.\n",
    "\n",
    "```\n",
    ">>> from __future__ import print_function\n",
    ">>> import sys\n",
    ">>> from operator import add\n",
    ">>>\n",
    ">>> # Load the lab3-iot-gendata.txt file for processing as a Resilient Distributed Dataset (RDD)\n",
    ">>> textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\")\n",
    ">>> \n",
    ">>> # Output the histogram\n",
    ">>> counts = textFile.flatMap(lambda x: x.split(' ')) \\\n",
    "...   .map(lambda x: (x, 1)) \\\n",
    "...   .reduceByKey(add)\n",
    ">>> output = counts.collect()\n",
    ">>> for (word, count) in output:\n",
    "...    print(\"%s: %i\" % (word, count))\n",
    "...\n",
    ">>> # Quit the shell\n",
    ">>> quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77: 5\n",
      ": 101\n",
      "84: 1\n",
      "76: 3\n",
      "68: 1\n",
      "81: 5\n",
      "82: 7\n",
      "103: 2\n",
      "85: 4\n",
      "73: 4\n",
      "63: 2\n",
      "79: 3\n",
      "86: 2\n",
      "101: 1\n",
      "62: 2\n",
      "87: 1\n",
      "74: 3\n",
      "102: 1\n",
      "100: 1\n",
      "104: 2\n",
      "88: 1\n",
      "60: 1\n",
      "65: 4\n",
      "98: 2\n",
      "92: 2\n",
      "90: 6\n",
      "78: 2\n",
      "94: 1\n",
      "93: 1\n",
      "96: 6\n",
      "95: 2\n",
      "64: 3\n",
      "71: 3\n",
      "70: 2\n",
      "61: 2\n",
      "89: 3\n",
      "97: 1\n",
      "75: 2\n",
      "83: 2\n",
      "91: 1\n",
      "99: 2\n",
      "67: 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\")\n",
    "\n",
    "counts = textFile.flatMap(lambda x : x.split(' ')) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(add)\n",
    "\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore Filter: Copy and paste these lines to see how to use Map, Filter, and Reduce functions in Spark. Copy and paste the session output to submit for this Lab.\n",
    "\n",
    "```\n",
    ">>> from __future__ import print_function\n",
    ">>> import sys\n",
    ">>> from operator import add\n",
    ">>>\n",
    ">>> textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\") \n",
    ">>> \n",
    ">>> # Output the filtered histogram \n",
    ">>> counts = textFile.flatMap(lambda x: x.split(' ')) \\\n",
    "... .filter(lambda x: x <= \"89\") \\\n",
    "... .map(lambda x: (x, 1)) \\\n",
    "... .reduceByKey(add)\n",
    ">>> output = counts.collect()\n",
    ">>> for (word, count) in output:\n",
    "...    print(\"%s: %i\" % (word, count))\n",
    "#####\n",
    "# NOTE 1: Take a screenshot/snapshot or copy and paste the session output to submit for this Lab.\n",
    "#####\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77: 5\n",
      ": 101\n",
      "84: 1\n",
      "76: 3\n",
      "68: 1\n",
      "81: 5\n",
      "82: 7\n",
      "103: 2\n",
      "85: 4\n",
      "73: 4\n",
      "63: 2\n",
      "79: 3\n",
      "86: 2\n",
      "101: 1\n",
      "62: 2\n",
      "87: 1\n",
      "74: 3\n",
      "102: 1\n",
      "100: 1\n",
      "104: 2\n",
      "88: 1\n",
      "60: 1\n",
      "65: 4\n",
      "78: 2\n",
      "64: 3\n",
      "71: 3\n",
      "70: 2\n",
      "61: 2\n",
      "89: 3\n",
      "75: 2\n",
      "83: 2\n",
      "67: 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\") \n",
    "\n",
    "# Output the filtered histogram \n",
    "counts = textFile.flatMap(lambda x: x.split(' ')) \\\n",
    "    .filter(lambda x: x <= \"89\") \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(add)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-316aab658838>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-316aab658838>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Ficing the issues with the example:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Ficing the issues with the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77: 5\n",
      "84: 1\n",
      "76: 3\n",
      "68: 1\n",
      "81: 5\n",
      "82: 7\n",
      "85: 4\n",
      "73: 4\n",
      "63: 2\n",
      "79: 3\n",
      "86: 2\n",
      "62: 2\n",
      "87: 1\n",
      "74: 3\n",
      "88: 1\n",
      "60: 1\n",
      "65: 4\n",
      "78: 2\n",
      "64: 3\n",
      "71: 3\n",
      "70: 2\n",
      "61: 2\n",
      "89: 3\n",
      "75: 2\n",
      "83: 2\n",
      "67: 1\n"
     ]
    }
   ],
   "source": [
    "# Output the filtered histogram \n",
    "counts = textFile.flatMap(lambda x: x.split()) \\\n",
    "    .filter(lambda x: int(x) <= 89) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(add)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = textFile.flatMap(lambda x: x.split()) \\\n",
    "    .filter(lambda x: (int(x) <= 89) && (int(x) >= 50) ) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(add)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
