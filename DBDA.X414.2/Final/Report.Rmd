---
title: "Spam Detection"
author: "Roque Arcudia, Arshia Razavi"
date: "March 22, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(magrittr)
```


## Motivation

## Data Set

The data set consists of….

Example of a few lines of the file

Cleanup: duplicates

## Dividing training and testing set

The strategy we follow is 80/20

## Text Vectorization

We can’t work directly with the spam data set as is. We need to map every entry to a vector format that is suitable for a machine learning algorithm. The approach we are going to follow is to convert each entry into a "bag of words".

The goal here is to build a Document-term matrix which is a mathematical representation of the training set showing the frequency of each word in each message

### Creating the vocabulary

As a first step we have to extract the vocabulary from the training set. A vocabulary is the set of unique words present in the given dataset and will be the basis for the construction of the DTM.

So if for instance we take this very small subset of our dataset:


```{r subset,echo=FALSE}
#cat testfile | awk '{ print length, $0 }' | sort -n -s | cut -d" " -f2-
example <- read.table("subset.txt",sep="\t",stringsAsFactors = FALSE,quote="",col.names=c("Class","Text"))
knitr::kable(example, caption = "Example")
```

The final vocabulary will look like this:

```{r example_voc,echo=FALSE}
example$ID <- seq.int(nrow(example))

it_example <- itoken(example$Text, 
             preprocessor = tolower, 
             tokenizer = word_tokenizer, 
             ids = example$ID, 
             progressbar = FALSE)
ex_voc  <- create_vocabulary(it_example)


knitr::kable(ex_voc, caption = "Example Vocabulary")
```

Notice how there is a conversion to lowercase, this is to avoid creating new entries for the same word and is part of our preprocessing. 

We relied on the r package “text2vec” to extract the vocabulary from the training set with the function “create_vocabulary”

### Document-term matrix 

From this point we can consider every word identified in the vocabulary as a single feature that will be part of the vectorized form of our dataset.

Each entry will be converted to a numerical representation in which each number will represents how many times a given element from the vocabulary was present in the message. This will create a matrix where the number of rows is the number of elements in the dataset and the number of columns is the number of elements in the vocabulary.

The Document-term matrix of our subset example will look like this

```{r example_dtm,echo=FALSE}

ex_dtm = create_dtm(it_example, vocab_vectorizer(ex_voc))

knitr::kable(as.matrix(ex_dtm), caption = "Example DTM", row.names = TRUE)

```

This example DTM has only 0s and 1s due to the small size of the inputs but in reality it is possible to find higher numbers.


We used the function “create_dtm” from “text2vec” to process the dataset and obtain the DTM. One of the parameters that has to be passed is a vectorizer function, in our case we used the “vocab_vectorizer” which is the one suitable to be used with a vocabulary.

### Real numbers

This is a raw extract from the vocabulary generated by text2vec applied to our training set.

The number of words is XXXX .

## Training Naive Bayes

The objective of this project is to create a spam classifier. The Naive Bayes classifier seems to be a good fit for this kind of projects. The idea is to classify a given text message by estimating the conditional probability of that particular message in its vectorized form $W$ being spam or ham and picking the class of the maximum value.


$$P(W|Ham)P(Ham) > P(W|Spam)P(Spam) \Rightarrow Ham$$
$$P(W|Ham)P(Ham) < P(W|Spam)P(Spam) \Rightarrow Spam$$
We estimate the conditional probability of a message by multiplying the individual conditional probabilities of each word that belongs to that message given the class (Spam or Ham). So for the example of the message “How come?” it will looks something like:

$$P(W=[how:1,come:1]|C)=P(how|C)P(come|C)$$

Our training process will consist of calculating the conditional probabilities for each word in the vocabulary given the class, where the classes are ham and spam. To calculate the probability of an individual word we count how many times that word appears across the given class and divide it by the total number of words in that given class.

$$P(word|C)=\frac{\text{Times word is in class C}}{\text{Total number of words in class C}}$$

### Implementation

We tried two naive bayes implementations from "naivebayes" and "e1071" but they didn’t work. It looks like whenever they see numerical matrices they try to use the mean and standard deviation approach. Because of these we implemented our own version of naive bayes based on counting words based on the book “Machine Learning in Action” chapter 4.

The basic algorithm we implemented goes like this:

1. Filter the input DTM using the class and separate into 2 new DTMs: DTMSpam and DTMHam.
2. Reduce each DTM into a single vector by applying column sum (colSums function in r). This will give a vector of the total number of a given words present in the class (the numerator).
3. To get the denominator value is as simple as applying a sum to each vector. This is a single number representing the total number of words per class.
4. Create a conditional probability per class vectors by dividing the numerator vector calculated in step 2 by the denominator calculated in step 3.
5. Calculate $P(Spam)$ by dividing the number of elements in the training set labeled as Spam by the total size of the training set. $P(Ham) = 1 - P(Spam)$

This is an example of how we would apply the sum per column in the DTM:

DTMSpam  | Word1 | Word2 | Word3 | ... | WordN
---------|-------|-------|-------|-----|-------
SMS1     |   1   |   0   |   0   | ... |   1
SMS2     |   0   |   2   |   0   | ... |   0
SMS3     |   0   |   1   |   0   | ... |   0
...      |  ...  |  ...  |  ...  | ... |  ...
SMSN     |   3   |   0   |   0   | ... |   2
**Total**| **8** | **7** | **0** | ... | **5**

We can easily spot that there will be some words that will have 0 count depending on the class. This will create a zero probability and everytime these words appear on a testing vector the total probability will become zero. To avoid this problem we apply some form of Laplace Smoothing by adding a count of 1 to every word. This will still keep those 0 count words as low probability but still allow other word calculation to be performed.

DTMSpam    | Word1 | Word2 | Word3 | ... | WordN
-----------|-------|-------|-------|-----|-------
Total      |   8   |   7   |   0   | ... |   5  
**Total+1**| **9** | **8** | **1** | ... | **6**


Another optimization suggested by the book is to take the natural logarithm of the probabilities. This would avoid the problem of small number multiplication and convert the multiplication to a simple sum.

$$\log(P(C)\prod_{i=1}^nP(w_i|C))=\log(P(C))+\sum_{i=1}^n\log(P(w_i|C))$$

We will be doing our comparisons in the logarithmic scale. Although the probability number will be affected in naive bayes we care about finding the class with the maximum probability. The natural logarithm operation won’t affect the comparisons since if one number is bigger than the other in the linear scale the same will still remain true for the logarithmic scale.

```{r logexample, echo=FALSE}
linearSeq <- seq(from=0.01,to=1,by=0.01)
testP <- c(0.4,0.6)

logSeq <- log(linearSeq)
logTestP <- log(testP)
```

If we take for example 2 probabilities P1=`r testP[1]` and P2=`r testP[2]` we know that P2>P1. In the logarithmic scale we have log(P1)=`r logTestP[1]` and log(P2)=`r logTestP[2]`, the relation log(P2)>log(P1) still remains true.

```{r logexampleplot, echo=FALSE}
par(mfcol=c(1,2))
plot(x=linearSeq,y=linearSeq,type="l",col="blue",main="Linear",ylab="P",xlab="")
points(x=testP[1],y=testP[1],col="red",pch=15)
points(x=testP[2],y=testP[2],col="red",pch=16)

plot(x=linearSeq,y=logSeq,type="l",col="blue",main="Logarithmic",ylab="log(P)",xlab="")
points(x=testP[1],y=logTestP[1],col="red",pch=15)
points(x=testP[2],y=logTestP[2],col="red",pch=16)
```


