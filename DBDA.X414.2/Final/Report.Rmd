---
title: "Spam Detection"
author: "Roque Arcudia, Arshia Razavi"
date: "March 22, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(magrittr)
library(glmnet)
library(caret)
source("bayes.r")

set.seed(1L)
```

## Introduction

We chose Spam Detection as the topic for this project mainly because of interest in knowing how text classification works. Spam is a very common problem in all forms of electronic communication nowadays. We searched online for spam datasets and the most common we found was the SMS spam data set that will be described in the next section.

This is a good introductory topic for Natural Language Processing and text based machine learning and the same approach can be used for different problems as document classification and sentiment analysis.

## Dataset

```{r readdata,echo=FALSE}

smsds <- readSmsSpamSource("SMSSpamCollection")

```

The dataset is a collection of “Ham” and “Spam” SMS messages from different sources. It was downloaded from UC Irvine Machine Learning repository. The following URL is the link where the dataset can be downloaded:

https://archive.ics.uci.edu/ml/datasets/sms+spam+collection

The dataset is a text file where there is one SMS per line. The line starts with the word “ham” or “spam” that classifies the message. The next thing found in the line is the SMS itself. The separator between the category and the message is a “tab” character. This is a sample of a few lines from the file, the spam messages are typically long so we picked a few that could fit the document:

```
ham	Ok lar... Joking wif u oni...
ham	U dun say so early hor... U c already then say...
ham	Nah I don't think he goes to usf, he lives around here though
ham	Even my brother is not like to speak with me. They treat me like aids patent.
spam	You have 1 new message. Call 0207-083-6089
spam	3. You have received your mobile content. Enjoy
spam	Mobile Club: Choose any of the top quality items for your mobile. 7cfca1a
spam	Money i have won wining number 946 wot do i do next
```

There is a total of `r length(smsds$isSpam)` lines and out of them `r sum(smsds$isSpam)` are spam.

## Training And Testing Sets

```{r divide_train_test,echo=FALSE}

ds <- divideTrainAndTest(smsds)
train <- ds$train
test <- ds$test

```

We used caret "createDataPartition" to create the training and testing set. We used 80% of the samples as training set and the remaining 20% as testing set. The function provides a random partition that has the same proportions of spam and ham as the original set.

Table: Summary of datasets

Set      | Size            | Spam                  | %Spam
---------|-----------------|-----------------------|---------------------------------
Whole    | `r nrow(smsds)` | `r sum(smsds$isSpam)` | `r 100*sum(smsds$isSpam)/nrow(smsds)`
Training | `r nrow(train)` | `r sum(train$isSpam)` | `r 100*sum(train$isSpam)/nrow(train)`
Testing  | `r nrow(test)`  | `r sum(test$isSpam)`  | `r 100*sum(test$isSpam)/nrow(test)`


## Text Vectorization

We can’t work directly with the spam data set as is. We need to map every entry to a vector format that is suitable for a machine learning algorithm. The approach we are going to follow is to convert each entry into a "bag of words".

The goal here is to build a Document-Term Matrix which is a mathematical representation of the training set showing the frequency of each word in each message

### Creating The Vocabulary

As a first step we have to extract the vocabulary from the training set. A vocabulary is the set of unique words present in the given dataset and will be the basis for the construction of the DTM.

If for instance we take this very small subset of our dataset:

```{r subset,echo=FALSE}
#cat testfile | awk '{ print length, $0 }' | sort -n -s | cut -d" " -f2-
example <- read.table("subset.txt",sep="\t",stringsAsFactors = FALSE,quote="",col.names=c("Class","Text"))
knitr::kable(example, caption = "Example dataset",row.names = TRUE)
```

The final vocabulary will look like this:

```{r example_voc,echo=FALSE}
example$ID <- seq.int(nrow(example))

it_example <- itoken(example$Text, 
             preprocessor = tolower, 
             tokenizer = word_tokenizer, 
             ids = example$ID, 
             progressbar = FALSE)
ex_voc  <- create_vocabulary(it_example)


knitr::kable(ex_voc, caption = "Example vocabulary")
```

Notice how there is a conversion to lowercase, this is to avoid creating new entries for the same word and is part of our preprocessing. 

We relied on the r package “text2vec” to extract the vocabulary from the training set with the function “create_vocabulary”

### Document-Term Matrix 

From this point we can consider every word identified in the vocabulary as a single feature that will be part of the vectorized form of our dataset.

Each entry will be converted to a numerical representation in which each number represents how many times a given element from the vocabulary was present in the message. This will create a matrix where the number of rows is the number of elements in the dataset and the number of columns is the number of elements in the vocabulary.

The Document-Term Matrix of our subset example will look like this

```{r example_dtm,echo=FALSE}

ex_dtm = create_dtm(it_example, vocab_vectorizer(ex_voc))

knitr::kable(as.matrix(ex_dtm), caption = "Example DTM", row.names = TRUE)

```

This example DTM has only 0s and 1s due to the small size of the inputs but in reality it is possible to find higher numbers.


We used the function “create_dtm” from “text2vec” to process the dataset and obtain the DTM. One of the parameters that has to be passed is a vectorizer function, in our case we used the “vocab_vectorizer” which is the one suitable to be used with a vocabulary.

### Real Numbers

This is a print out from the vocabulary generated by text2vec applied to our training set:

```{r vocab_and_dtms,echo=FALSE}
textVec <- createVocabularyAndDTMs(train,test)
dtm_train_matrix <- as.matrix(textVec$trainDTM)
dtm_test_matrix <- as.matrix(textVec$testDTM)

textVec$vocab
```

We can see that our vocabulary in this particular run consists of `r length(textVec$vocab$term)` words. This would be the number of columns in our DTMs. The "Number of docs" is the number of rows in the training DTM and we know already is the number of rows in the training set: `r attr(textVec$vocab,"document_count")`.

## Training Naive Bayes

The objective of this project is to create a spam classifier. The Naive Bayes classifier seems to be a good fit for this kind of projects. The idea is to classify a given text message by estimating the conditional probability of that particular message in its vectorized form $W$ being spam or ham and picking the class of the maximum value.

$$P(W|Ham)P(Ham) > P(W|Spam)P(Spam) \Rightarrow Ham$$
$$P(W|Ham)P(Ham) < P(W|Spam)P(Spam) \Rightarrow Spam$$
We estimate the conditional probability of a message by multiplying the individual conditional probabilities of each word that belongs to that message given the class (Spam or Ham). So for the example of the message “How come?” it will look something like:

$$P(W=[how:1,come:1]|C)=P(how|C)P(come|C)$$

Our training process will consist of calculating the conditional probabilities for each word in the vocabulary given the class, where the classes are ham and spam. To calculate the probability of an individual word we count how many times that word appears across the given class and divide it by the total number of words in that given class.

$$P(word|C)=\frac{\text{Times word is in class C}}{\text{Total number of words in class C}}$$

### Implementation

We tried two naive bayes implementations from "naivebayes"" and "e1071"" but they didn't work. It looks like whenever they see numerical matrices they try to use the mean and standard deviation approach. Because of these we implemented our own version of naive bayes based on counting words. We got the algorithm from the book “Machine Learning in Action” chapter 4 and we did a much simpler R implementation.

The basic algorithm we implemented goes like this:

1. Filter the input DTM using the class and separate into 2 new DTMs: DTMSpam and DTMHam.
2. Reduce each DTM into a single vector by applying column sum (colSums function in r). This will give a vector of word counts per class (the numerator).
3. To get the denominator value is as simple as applying a sum to each result vector from step 2. This is a single number representing the total number of words per class.
4. Create the conditional probability per class vectors by dividing the numerator vectors calculated in step 2 by the denominators calculated in step 3.
5. Calculate $P(Spam)$ by dividing the number of elements in the training set labeled as Spam by the total size of the training set. $P(Ham) = 1 - P(Spam)$

This is an example of how we would apply the sum per column in the DTM:

Table: Column sum on spam DTM example

DTMSpam  | Word1 | Word2 | Word3 | ... | WordN
---------|-------|-------|-------|-----|-------
SMS1     |   1   |   0   |   0   | ... |   1
SMS2     |   0   |   2   |   0   | ... |   0
SMS3     |   0   |   1   |   0   | ... |   0
...      |  ...  |  ...  |  ...  | ... |  ...
SMSN     |   3   |   0   |   0   | ... |   2
**Total**| **8** | **7** | **0** | ... | **5**

We can easily spot that there will be some words that will have 0 count depending on the class. This will create a zero probability and every time these words appear on a testing vector the total probability will become zero. To avoid this problem we apply some form of Laplace Smoothing by adding a count of 1 to every word. This will still keep those 0 count words as low probability but still allow other word calculation to be performed.

Table: Adding 1 to avoid the 0 probability

DTMSpam    | Word1 | Word2 | Word3 | ... | WordN
-----------|-------|-------|-------|-----|-------
Total      |   8   |   7   |   0   | ... |   5  
**Total+1**| **9** | **8** | **1** | ... | **6**


Another optimization suggested by the book is to take the natural logarithm of the probabilities. This would avoid the problem of small number multiplication and convert the multiplication to a simple sum.

$$\log(P(C)\prod_{i=1}^nP(w_i|C))=\log(P(C))+\sum_{i=1}^n\log(P(w_i|C))$$

We will be doing our comparisons in the logarithmic scale. Although the probability number will be affected, in Naive Bayes we care about finding the class with the maximum probability. The natural logarithm operation won’t affect the comparisons since if one number is bigger than the other in the linear scale the same will still remain true for the logarithmic scale.

```{r logexample, echo=FALSE}
linearSeq <- seq(from=0.01,to=1,by=0.01)
testP <- c(0.4,0.6)

logSeq <- log(linearSeq)
logTestP <- log(testP)
```

If we take for example 2 probabilities $P_1$=`r testP[1]` and $P_2$=`r testP[2]` we know that $P_2>P_1$. In the logarithmic scale we have $\log(P_1)=$`r logTestP[1]` and $\log(P_2)$=`r logTestP[2]`, the relation $\log(P_2)>\log(P_1)$ still remains true.

```{r logexampleplot, echo=FALSE}
par(mfcol=c(1,2))
plot(x=linearSeq,y=linearSeq,type="l",col="blue",main="Linear",ylab="P",xlab="")
points(x=testP[1],y=testP[1],col="red",pch=15)
points(x=testP[2],y=testP[2],col="red",pch=16)

plot(x=linearSeq,y=logSeq,type="l",col="blue",main="Logarithmic",ylab="log(P)",xlab="")
points(x=testP[1],y=logTestP[1],col="red",pch=15)
points(x=testP[2],y=logTestP[2],col="red",pch=16)
```

At the end of the training phase we have three things:

```{r train_model,echo=FALSE}

nbModel <- trainNaiveBayes(dtm_train_matrix,train$isSpam)

```
1. The probability of being "spam" $P(Spam)$. The "ham" one is just the complement.
2. A vector containing for each word in the vocabulary the conditional probability $\log(P(w_i|Spam))$. The vector will be column consistent with our training DTM.
3. A vector containing for each word in the vocabulary the conditional probability $\log(P(w_i|Ham))$. The vector will be column consistent with our training DTM.

### Probability Vectors

If we look at the top 10 probabilities in the vector $\log(P(w_i|Ham))$, we can see there are only very common words as we expect.

```{r psham,echo=FALSE}
pNegOrd <- order(-nbModel$pNegVectLog)
knitr::kable(head(nbModel$pNegVectLog[pNegOrd],10), caption = "Ham: Words with higher probability", row.names = TRUE)
```

Now if we look at the top 10 probabilities in the $\log(P(w_i|Spam))$ vector, we can see there is a mixture of common words and some keywords that are really common to the spam messages for instance: call, free, now.

```{r pspam,echo=FALSE}
pPosOrd <- order(-nbModel$pPosVectLog)
knitr::kable(head(nbModel$pPosVectLog[pPosOrd],10), caption = "Spam: Words with higher probability", row.names = TRUE)
```

From observing a few spam messages it is common in them to offer something for free and then request you to reply something or call a phone number.

## Applying Naive Bayes Model

Now that we have our model trained the next step is to apply the model to our testing set in order to get some metrics. This is the output of caret confusionMatrix function:

```{r apply_model,echo=FALSE}

numTestDocs <- nrow(dtm_test_matrix)
results <- logical(numTestDocs)

for ( i in 1:numTestDocs) {
  testvec <- dtm_test_matrix[i,]
  results[i] <- predictNaiveBayes(nbModel,testvec)
}


#### Models Performance ####

cm <- confusionMatrix(results,reference=test$isSpam,positive="TRUE")

cm
```

The Accuracy of our classifier was `r cm$overall["Accuracy"]` which is really good. The Sensitivity (`r cm$byClass["Sensitivity"]`) and Positive Predictive Value (`r cm$byClass["Pos Pred Value"]`) are lower compared to other metrics because they depend on the spam classification, with not many spam rows in our testing set the errors are proportionally bigger.

## Logistic Regression

Just as a plus we used our DTMs to train a logistic regression model and test the model performance too. The logistic regression model we used is from the library “glmnet” which provides a very fast training of the model. This is the model used in the tutorials for the package “text2vect” so we used the same parameters as in the tutorial.

The final confusion matrix after training the model and applying the model to the testing set is:

```{r logregmod, echo=FALSE}
modelLR <- trainLogisticRegression(textVec$trainDTM, train$isSpam)

resultsLR <- predictLogisticRegression(modelLR,textVec$testDTM)

cmLR <- confusionMatrix(resultsLR>0.5,reference=test$isSpam,positive="TRUE")
cmLR
```

The model still has good accuracy: `r cmLR$overall["Accuracy"]`. This particular run experienced an increase in the False Negative results so the Sensitivity was penalized  (`r cmLR$byClass["Sensitivity"]`) but in general we could say results are close to Bayes.

## Conclusion

Naive Bayes provides good accuracy and it is very simple and relatively fast. As we saw the model suffers from low Sensitivity and PPV given the proportionally high error numbers compared to the spam examples in the testing set.

We believe this works really well because, as we saw in the probability vector of spam, the spam set has a few keywords that repeat in a lot of messages that makes easy to identify spam: call, free, now, reply, etc. The presence of any of these keywords will increase the probability of being spam but will have minimal effect in the probability of being ham.

The logistic regression model run for comparison also gives good results. It is probable that the final equation is getting benefited from the spam keywords too.

## References

- Harrington, Peter. *Machine Learning in Action*. Manning, 2012. Chapter 4
- CRAN page for text2vec: https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html


