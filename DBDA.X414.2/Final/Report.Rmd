---
title: "Spam Detection"
author: "Roque Arcudia, Arshia Razavi"
date: "March 22, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(magrittr)
```


## Motivation

## Data Set

The data set consists of….

Example of a few lines of the file

Cleanup: duplicates

## Dividing training and testing set

The strategy we follow is 80/20

## Text Vectorization

We can’t work directly with the spam data set as is. We need to map every entry to a vector format that is suitable for a machine learning algorithm. The approach we are going to follow is to convert each entry into a "bag of words".

The goal here is to build a Document-term matrix which is a mathematical representation of the training set showing the frequency of each word in each message

### Creating the vocabulary

As a first step we have to extract the vocabulary from the training set. A vocabulary is the set of unique words present in the given dataset and will be the basis for the construction of the DTM.

So if for instance we take this very small subset of our dataset:


```{r subset,echo=FALSE}
#cat testfile | awk '{ print length, $0 }' | sort -n -s | cut -d" " -f2-
example <- read.table("subset.txt",sep="\t",stringsAsFactors = FALSE,quote="",col.names=c("Class","Text"))
knitr::kable(example, caption = "Example")
```

The final vocabulary will look like this:

```{r example_voc,echo=FALSE}
example$ID <- seq.int(nrow(example))

it_example <- itoken(example$Text, 
             preprocessor = tolower, 
             tokenizer = word_tokenizer, 
             ids = example$ID, 
             progressbar = FALSE)
ex_voc  <- create_vocabulary(it_example)


knitr::kable(ex_voc, caption = "Example Vocabulary")
```

Notice how there is a conversion to lowercase, this is to avoid creating new entries for the same word and is part of our preprocessing. 

We relied on the r package “text2vec” to extract the vocabulary from the training set with the function “create_vocabulary”

### Document-term matrix 

From this point we can consider every word identified in the vocabulary as a single feature that will be part of the vectorized form of our dataset.

Each entry will be converted to a numerical representation in which each number will represents how many times a given element from the vocabulary was present in the message. This will create a matrix where the number of rows is the number of elements in the dataset and the number of columns is the number of elements in the vocabulary.

The Document-term matrix of our subset example will look like this

```{r example_dtm,echo=FALSE}

ex_dtm = create_dtm(it_example, vocab_vectorizer(ex_voc))

knitr::kable(as.matrix(ex_dtm), caption = "Example DTM", row.names = TRUE)

```

This example DTM has only 0s and 1s due to the small size of the inputs but in reality it is possible to find higher numbers.


We used the function “create_dtm” from “text2vec” to process the dataset and obtain the DTM. One of the parameters that has to be passed is a vectorizer function, in our case we used the “vocab_vectorizer” which is the one suitable to be used with a vocabulary.

### Real numbers

This is a raw extract from the vocabulary generated by text2vec applied to our training set.

The number of words is XXXX .

## Naive Bayes

The objective of this project is to create a spam classifier. The Naive Bayes classifier seems to be a good fit for this kind of projects. The idea is to classify a given text message by estimating the conditional probability of that particular message in its vectorized form $W$ being spam or ham and picking the class of the maximum value.


$$P(W|Ham)P(Ham) > P(W|Spam)P(Spam) \Rightarrow Ham$$
$$P(W|Ham)P(Ham) < P(W|Spam)P(Spam) \Rightarrow Spam$$
We estimate the conditional probability of a message by multiplying the individual conditional probabilities of each word that belongs to that message given the class (Spam or Ham). So for the example of the message “How come?” it will looks something like:

$$P(W=[how:1,come:1]|C)=P(how|C)P(come|C)$$

Our training process will consist of calculating the conditional probabilities for each word in the vocabulary given the class, where the classes are ham and spam. To calculate the probability of an individual word we count how many times that word appears across the given class and divide it by the total number of words in that given class.

$$P(word|C)=\frac{\text{Times word is in C}}{\text{Total number of words in C}}$$



$$p(X|C_k) = \prod_{i=1}^np(x_i|C_k)$$