---
title: "Spam Detection"
author: "Roque Arcudia, Arshia Razavi"
date: "March 22, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(magrittr)
source("bayes.r")

set.seed(1L)
```


## Motivation

## Dataset

```{r readdata,echo=FALSE}

smsds <- readSmsSpamSource("SMSSpamCollection")

```

The dataset is a collection of “Ham” and “Spam” SMS messages from different sources. It was downloaded from UC Irvine Machine Learning repository. The following URL is the link where the dataset can be downloaded:

https://archive.ics.uci.edu/ml/datasets/sms+spam+collection

The dataset is a text file where there is one SMS per line. The line starts with the word “ham” or “spam” that classifies the message. The next thing found in the line is the SMS itself. The separator between the category and the message is a “tab” character. This is a sample of a few lines from the file, the spam messages are typically long so we picked a few that could fit the document:

```
ham	Ok lar... Joking wif u oni...
ham	U dun say so early hor... U c already then say...
ham	Nah I don't think he goes to usf, he lives around here though
ham	Even my brother is not like to speak with me. They treat me like aids patent.
spam	You have 1 new message. Call 0207-083-6089
spam	3. You have received your mobile content. Enjoy
spam	Mobile Club: Choose any of the top quality items for your mobile. 7cfca1a
spam	Money i have won wining number 946 wot do i do next
```

There is a total of `r length(smsds$isSpam)` lines and out of them `r sum(smsds$isSpam)` are spam.

## Cleanup: duplicates

## Dividing Training And Testing Sets

```{r divide_train_test,echo=FALSE}

ds <- divideTrainAndTest(smsds)
train <- ds$train
test <- ds$test

```

We used caret createDataPartition to create the training and testing set. We used 80% of the samples as training set and the remaining 20% as testing set. The function provides a random partition that has the same proportions of spam and ham as the original set.

Table: Summary of datasets

Set      | Size            | Spam                  | %Spam
---------|-----------------|-----------------------|---------------------------------
Whole    | `r nrow(smsds)` | `r sum(smsds$isSpam)` | `r 100*sum(smsds$isSpam)/nrow(smsds)`
Training | `r nrow(train)` | `r sum(train$isSpam)` | `r 100*sum(train$isSpam)/nrow(train)`
Testing  | `r nrow(test)`  | `r sum(test$isSpam)`  | `r 100*sum(test$isSpam)/nrow(test)`


## Text Vectorization

We can’t work directly with the spam data set as is. We need to map every entry to a vector format that is suitable for a machine learning algorithm. The approach we are going to follow is to convert each entry into a "bag of words".

The goal here is to build a Document-term matrix which is a mathematical representation of the training set showing the frequency of each word in each message

### Creating The Vocabulary

As a first step we have to extract the vocabulary from the training set. A vocabulary is the set of unique words present in the given dataset and will be the basis for the construction of the DTM.

So if for instance we take this very small subset of our dataset:


```{r subset,echo=FALSE}
#cat testfile | awk '{ print length, $0 }' | sort -n -s | cut -d" " -f2-
example <- read.table("subset.txt",sep="\t",stringsAsFactors = FALSE,quote="",col.names=c("Class","Text"))
knitr::kable(example, caption = "Example dataset",row.names = TRUE)
```

The final vocabulary will look like this:

```{r example_voc,echo=FALSE}
example$ID <- seq.int(nrow(example))

it_example <- itoken(example$Text, 
             preprocessor = tolower, 
             tokenizer = word_tokenizer, 
             ids = example$ID, 
             progressbar = FALSE)
ex_voc  <- create_vocabulary(it_example)


knitr::kable(ex_voc, caption = "Example vocabulary")
```

Notice how there is a conversion to lowercase, this is to avoid creating new entries for the same word and is part of our preprocessing. 

We relied on the r package “text2vec” to extract the vocabulary from the training set with the function “create_vocabulary”

### Document-Term Matrix 

From this point we can consider every word identified in the vocabulary as a single feature that will be part of the vectorized form of our dataset.

Each entry will be converted to a numerical representation in which each number will represents how many times a given element from the vocabulary was present in the message. This will create a matrix where the number of rows is the number of elements in the dataset and the number of columns is the number of elements in the vocabulary.

The Document-term matrix of our subset example will look like this

```{r example_dtm,echo=FALSE}

ex_dtm = create_dtm(it_example, vocab_vectorizer(ex_voc))

knitr::kable(as.matrix(ex_dtm), caption = "Example DTM", row.names = TRUE)

```

This example DTM has only 0s and 1s due to the small size of the inputs but in reality it is possible to find higher numbers.


We used the function “create_dtm” from “text2vec” to process the dataset and obtain the DTM. One of the parameters that has to be passed is a vectorizer function, in our case we used the “vocab_vectorizer” which is the one suitable to be used with a vocabulary.

### Real Numbers

```{r vocab_and_dtms,echo=FALSE}
textVec <- createVocabularyAndDTMs(train,test)
dtm_train_matrix <- as.matrix(textVec$trainDTM)
dtm_test_matrix <- as.matrix(textVec$testDTM)

textVec$vocab
```
This is a raw extract from the vocabulary generated by text2vec applied to our training set.

The number of words is XXXX .

## Training Naive Bayes

The objective of this project is to create a spam classifier. The Naive Bayes classifier seems to be a good fit for this kind of projects. The idea is to classify a given text message by estimating the conditional probability of that particular message in its vectorized form $W$ being spam or ham and picking the class of the maximum value.


$$P(W|Ham)P(Ham) > P(W|Spam)P(Spam) \Rightarrow Ham$$
$$P(W|Ham)P(Ham) < P(W|Spam)P(Spam) \Rightarrow Spam$$
We estimate the conditional probability of a message by multiplying the individual conditional probabilities of each word that belongs to that message given the class (Spam or Ham). So for the example of the message “How come?” it will look something like:

$$P(W=[how:1,come:1]|C)=P(how|C)P(come|C)$$

Our training process will consist of calculating the conditional probabilities for each word in the vocabulary given the class, where the classes are ham and spam. To calculate the probability of an individual word we count how many times that word appears across the given class and divide it by the total number of words in that given class.

$$P(word|C)=\frac{\text{Times word is in class C}}{\text{Total number of words in class C}}$$

### Implementation

We tried two naive bayes implementations from "naivebayes"" and "e1071"" but they didn't work. It looks like whenever they see numerical matrices they try to use the mean and standard deviation approach. Because of these we implemented our own version of naive bayes based on counting words. We got the algorithm from the book “Machine Learning in Action” chapter 4 and we did a much simpler R implementation.

The basic algorithm we implemented goes like this:

1. Filter the input DTM using the class and separate into 2 new DTMs: DTMSpam and DTMHam.
2. Reduce each DTM into a single vector by applying column sum (colSums function in r). This will give a vector of the total number of a given words present in the class (the numerator).
3. To get the denominator value is as simple as applying a sum to each result vector from step 2. This is a single number representing the total number of words per class.
4. Create a conditional probability per class vectors by dividing the numerator vector calculated in step 2 by the denominator calculated in step 3.
5. Calculate $P(Spam)$ by dividing the number of elements in the training set labeled as Spam by the total size of the training set. $P(Ham) = 1 - P(Spam)$

This is an example of how we would apply the sum per column in the DTM:

DTMSpam  | Word1 | Word2 | Word3 | ... | WordN
---------|-------|-------|-------|-----|-------
SMS1     |   1   |   0   |   0   | ... |   1
SMS2     |   0   |   2   |   0   | ... |   0
SMS3     |   0   |   1   |   0   | ... |   0
...      |  ...  |  ...  |  ...  | ... |  ...
SMSN     |   3   |   0   |   0   | ... |   2
**Total**| **8** | **7** | **0** | ... | **5**

We can easily spot that there will be some words that will have 0 count depending on the class. This will create a zero probability and everytime these words appear on a testing vector the total probability will become zero. To avoid this problem we apply some form of Laplace Smoothing by adding a count of 1 to every word. This will still keep those 0 count words as low probability but still allow other word calculation to be performed.

DTMSpam    | Word1 | Word2 | Word3 | ... | WordN
-----------|-------|-------|-------|-----|-------
Total      |   8   |   7   |   0   | ... |   5  
**Total+1**| **9** | **8** | **1** | ... | **6**


Another optimization suggested by the book is to take the natural logarithm of the probabilities. This would avoid the problem of small number multiplication and convert the multiplication to a simple sum.

$$\log(P(C)\prod_{i=1}^nP(w_i|C))=\log(P(C))+\sum_{i=1}^n\log(P(w_i|C))$$

We will be doing our comparisons in the logarithmic scale. Although the probability number will be affected in naive bayes we care about finding the class with the maximum probability. The natural logarithm operation won’t affect the comparisons since if one number is bigger than the other in the linear scale the same will still remain true for the logarithmic scale.

```{r logexample, echo=FALSE}
linearSeq <- seq(from=0.01,to=1,by=0.01)
testP <- c(0.4,0.6)

logSeq <- log(linearSeq)
logTestP <- log(testP)
```

If we take for example 2 probabilities $P_1$=`r testP[1]` and $P_2$=`r testP[2]` we know that $P_2>P_1$. In the logarithmic scale we have $\log(P_1)=$`r logTestP[1]` and $\log(P_2)$=`r logTestP[2]`, the relation $\log(P_2)>\log(P_1)$ still remains true.

```{r logexampleplot, echo=FALSE}
par(mfcol=c(1,2))
plot(x=linearSeq,y=linearSeq,type="l",col="blue",main="Linear",ylab="P",xlab="")
points(x=testP[1],y=testP[1],col="red",pch=15)
points(x=testP[2],y=testP[2],col="red",pch=16)

plot(x=linearSeq,y=logSeq,type="l",col="blue",main="Logarithmic",ylab="log(P)",xlab="")
points(x=testP[1],y=logTestP[1],col="red",pch=15)
points(x=testP[2],y=logTestP[2],col="red",pch=16)
```

At the end of the training phase we have three things:

```{r train_model,echo=FALSE}

nbModel <- trainNaiveBayes(dtm_train_matrix,train$isSpam)

```
1. The probability of being "spam" $P(spam)$. The "ham" one is just the complement.
2. A vector containing for each word in the vocabulary the conditional probability $\log(P(word|spam))$. The vector will be column consistent with our training DTM.
3. A vector containing for each word in the vocabulary the conditional probability $\log(P(word|ham))$. The vector will be column consistent with our training DTM.

## Applying Naive Bayes Model

Now that we have our model trained the next step is to apply the model to our testing set in order to get some metrics. This is the output of caret confusionMatrix function:

```{r apply_model,echo=FALSE}

numTestDocs <- nrow(dtm_test_matrix)
results <- logical(numTestDocs)

for ( i in 1:numTestDocs) {
  testvec <- dtm_test_matrix[i,]
  results[i] <- predictNaiveBayes(nbModel,testvec)
}


#### Models Performance ####

cm <- confusionMatrix(results,reference=test$isSpam,positive="TRUE")

cm
```

## Log Reg

```{r logregmod}
# Ultra slow:
#lrm <- glm.fit(x = dtm_train_matrix, y = train$isSpam, family = binomial())

library(glmnet)
NFOLDS = 4

glmnet_classifier = cv.glmnet(x = textVec$trainDTM, y = train$isSpam, 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
```

Predict
```{r predict}
#preds = predict(lrm, dtm_test_matrix)
#preds

preds = predict(glmnet_classifier, textVec$testDTM, type = 'response')[,1]
#preds = predict(glmnet_classifier, textVec$testDTM) > 0
#preds
```
Confusion matrix

```{r }
cmLr <- confusionMatrix(preds>0.5,reference=test$isSpam,positive="TRUE")
cmLr
```

