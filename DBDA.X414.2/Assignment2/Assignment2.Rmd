---
title: "ASSIGNMENT 2"
author: "Roque Arcudia"
date: "February 26, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**1. What assumptions are made about the attributes in Naïve Bayes method of classification and why? What is Laplacian correction and why it is necessary?**

**2.How can a decision tree be converted into a rule set? Illustrate with an example. What are the advantages of the rule set representation over the decision tree representation?**

**3.Take the iris data. Use Naïve Bayes algorithm to find the species with the following two observations with attributes
[4.5 3.0 5.6 2.1; 5.4 2.6 4.5 0.0] Don’t use the any R package. Show
various steps involved and do as explained in the class.**

Importing iris dataset and dplyr for better dataframe handling

```{r iris_imports,message=FALSE}
library(datasets)
library(dplyr)
data(iris)
```

Calculating the mean and the variance for each feature for each class.

```{r iris_summary}
iris.gp.by.Species <- iris %>% group_by(Species)

iris.mean.gp.by.Species <- iris.gp.by.Species %>% summarise_all(mean)
knitr::kable(iris.mean.gp.by.Species,caption = "Means by class")

iris.var.gp.by.Species <- iris.gp.by.Species %>% summarise_all(var)
knitr::kable(iris.var.gp.by.Species, caption = "Variances by class")
```

Calculate the probability of each class.

```{r iris_prob}
ntotal <- nrow(iris)
iris.Prob.gp.by.Species <- iris.gp.by.Species %>%
  summarise(Prob=n()/ntotal)
knitr::kable(iris.Prob.gp.by.Species, caption = "P(class)")
```

Preparing the input test vectors for classification.

```{r test_vectors}
feature_names <- names(iris)
feature_names <- feature_names[feature_names!="Species"]
test.data <- as.data.frame(
  matrix(c(4.5, 3.0, 5.6, 2.1, 5.4, 2.6, 4.5, 0.0),nrow=2,byrow=TRUE)
  )
names(test.data) <- feature_names
vector_names <- sapply(c(1:nrow(test.data)),function(x) {paste("Vector",x)})
row.names(test.data)<-vector_names
knitr::kable(test.data,caption = "Vectors")
```

Calculating the PDFs using a normal distribution using the mean and variance previously calculated. Creating my own function taking the input vector and a vector of mean and variance.

```{r test_pdf}
calculateNorm <- function(x,input.mean,input.var) {
  return(exp((-(x-input.mean)^2)/(2*input.var))/sqrt(2*pi*input.var))
}

calculatePdfs <- function(input.data) {
  species_names <- iris.Prob.gp.by.Species$Species
  pdfs <- sapply(iris.Prob.gp.by.Species$Species, function(x) {
    this.mean <- iris.mean.gp.by.Species[
      iris.mean.gp.by.Species$Species==x,
      feature_names
      ]
    this.var <- iris.var.gp.by.Species[
        iris.mean.gp.by.Species$Species==x,
        feature_names
        ]
    # Future summarise doesn't work if it is not numeric
    #pdf <- calculateNorm((input.data),(this.mean),(this.var))
    pdf <- calculateNorm(
      as.numeric(input.data),
      input.mean=as.numeric(this.mean),
      input.var=as.numeric(this.var)
      )
    return(pdf)
  })
  rownames(pdfs) <- feature_names
  colnames(pdfs) <- species_names
  return(pdfs)
}

pdfs.per.vector <- list()

for(i in 1:nrow(test.data)){
  pdfs <- calculatePdfs(test.data[i,])
  pdfs
  pdfs.per.vector[[i]] <- pdfs
}

pdfs.per.vector
```

The table will be read the value of the PDF of “row name” given “column name” for instance p(Sepal.Length=`r test.data[1,1]`|setosa)=`r pdfs.per.vector[[1]][1,1]`

Multiplying the results of all the PDF values by the class probability too and picking up the class of the highest number. For each class (setosa, versicolor and virginica) we calculate:

$$P(class)p(Sepal.Length|class)p(Sepal.Width|class)p(Petal.Length|class)p(Petal.Width|class)$$

```{r iris_class}
posteriors.per.vector<- t(sapply(pdfs.per.vector, function(input.pdfs) {
  probs <- rbind(input.pdfs,iris.Prob.gp.by.Species$Prob)
  posteriors <- as.data.frame(probs) %>% summarise_all(prod)
  winner <- names(posteriors)[which.max(posteriors)]
  return(posteriors %>% mutate(winner=winner))
}))

row.names(posteriors.per.vector)<-vector_names
knitr::kable(posteriors.per.vector,caption = "Classification")

```

The calculation of the “evidence” is omitted since will be a constant and won’t affect the result of the comparison.

```{r iris_extraction, include=FALSE}
# Deprecated code, new une uses dplyr and more compact
library(datasets)
data(iris)

ntotal <- nrow(iris)
setosa_total <- sum(iris$Species=="setosa")
versicolor_total <- sum(iris$Species=="versicolor")
virginica_total <- sum(iris$Species=="virginica")

ntotal
setosa_total
versicolor_total
virginica_total

P_setosa<- setosa_total/ntotal
P_versicolor<- versicolor_total/ntotal
P_virginica<- virginica_total/ntotal

P_setosa
P_versicolor
P_virginica

feature_names <- names(iris)
feature_names <- feature_names[feature_names!="Species"]
feature_names
#num_cols <- names(iris)!="Species"
iris_setosa <- iris[iris$Species=="setosa",feature_names]
iris_versicolor <- iris[iris$Species=="versicolor",feature_names]
iris_virginica <- iris[iris$Species=="virginica",feature_names]
  
iris_setosa
iris_versicolor
iris_virginica
summary(iris_setosa)
summary(iris_versicolor)
summary(iris_virginica)

#iris_setosa_mean <-colMeans(iris_setosa)
#iris_versicolor_mean <- colMeans(iris_versicolor)
#iris_virginica_mean <- colMeans(iris_virginica)

iris_setosa_mean <-sapply(iris_setosa,mean)
iris_versicolor_mean <- sapply(iris_versicolor,mean)
iris_virginica_mean <- sapply(iris_virginica,mean)

iris_setosa_mean
iris_versicolor_mean
iris_virginica_mean

iris_setosa_var <-sapply(iris_setosa,var)
iris_versicolor_var <- sapply(iris_versicolor,var)
iris_virginica_var <- sapply(iris_virginica,var)

iris_setosa_var
iris_versicolor_var
iris_virginica_var

#[4.5 3.0 5.6 2.1; 5.4 2.6 4.5 0.0]

my_norm <- function(x,meanx,varx) {
	return(exp((-(x-meanx)^2)/(2*varx))/sqrt(2*pi*varx))
}
#Sepal.Length
p_Sepal.Length_setosa <- my_norm(4.5, iris_setosa_mean['Sepal.Length'],iris_setosa_var['Sepal.Length'])
p_Sepal.Length_versicolor <- my_norm(4.5, iris_versicolor_mean['Sepal.Length'],iris_versicolor_var['Sepal.Length'])
p_Sepal.Length_virginica <- my_norm(4.5, iris_virginica_mean['Sepal.Length'],iris_virginica_var['Sepal.Length'])

p_Sepal.Length_setosa
p_Sepal.Length_versicolor
p_Sepal.Length_virginica

#Sepal.Width
p_Sepal.Width_setosa <- my_norm(3.0, iris_setosa_mean['Sepal.Width'],iris_setosa_var['Sepal.Width'])
p_Sepal.Width_versicolor <- my_norm(3.0, iris_versicolor_mean['Sepal.Width'],iris_versicolor_var['Sepal.Width'])
p_Sepal.Width_virginica <- my_norm(3.0, iris_virginica_mean['Sepal.Width'],iris_virginica_var['Sepal.Width'])

p_Sepal.Width_setosa
p_Sepal.Width_versicolor
p_Sepal.Width_virginica




#Petal.Length
p_Petal.Length_setosa <- my_norm(5.6, iris_setosa_mean['Petal.Length'],iris_setosa_var['Petal.Length'])
p_Petal.Length_versicolor <- my_norm(5.6, iris_versicolor_mean['Petal.Length'],iris_versicolor_var['Petal.Length'])
p_Petal.Length_virginica <- my_norm(5.6, iris_virginica_mean['Petal.Length'],iris_virginica_var['Petal.Length'])

p_Petal.Length_setosa
p_Petal.Length_versicolor
p_Petal.Length_virginica

#Petal.Width
p_Petal.Width_setosa <- my_norm(2.1, iris_setosa_mean['Petal.Width'],iris_setosa_var['Petal.Width'])
p_Petal.Width_versicolor <- my_norm(2.1, iris_versicolor_mean['Petal.Width'],iris_versicolor_var['Petal.Width'])
p_Petal.Width_virginica <- my_norm(2.1, iris_virginica_mean['Petal.Width'],iris_virginica_var['Petal.Width'])

p_Petal.Width_setosa
p_Petal.Width_versicolor
p_Petal.Width_virginica

#Goldens:
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##        5.006        3.428        1.462        0.246
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##        5.936        2.770        4.260        1.326
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##        6.588        2.974        5.552        2.026

## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##   0.12424898   0.14368980   0.03015918   0.01110612
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##   0.26643265   0.09846939   0.22081633   0.03910612
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##   0.40434286 0.10400408 0.30458776 0.07543265


posterior_setosa <- P_setosa * p_Sepal.Length_setosa * p_Sepal.Width_setosa * p_Petal.Length_setosa * p_Petal.Width_setosa
posterior_versicolor <- P_versicolor * p_Sepal.Length_versicolor * p_Sepal.Width_versicolor * p_Petal.Length_versicolor * p_Petal.Width_versicolor
posterior_virginica <- P_virginica * p_Sepal.Length_virginica * p_Sepal.Width_virginica * p_Petal.Length_virginica * p_Petal.Width_virginica

posterior_setosa
posterior_versicolor
posterior_virginica

## 2.093198e-191

```



**1.What is machine learning? What is the difference between data mining and machine learning? (2 points)**

Machine learning is the area of computer science that designs algorithms to learn patterns from historical data. It is based on training the algorithm with an input training set typically using statistical approaches. The algorithm should be able to generalize.

Data mining is in charge of discovering patterns and finding structure in a large and unstructured data sets. Machine Learning could be used as a tool for data mining but it also involve the use of other tools and techniques as database systems, visualization tools, etc.

**2. What is supervised and unsupervised learning? Give examples and applications where each can be used (2 points)**

In supervised learning the input training set is labeled. The goal is to find or approximate a function that best describes the relationship between the inputs and the outputs (labels). Supervised learning can be further divided into classification if the output is a set of discrete values and regression if the output is a continuous value.

Some examples would be credit card fraud detection, spam detection and image recognition in case of classification. An example of regression would be predicting stock value based on historical data.

In unsupervised learning the training data is given without labels. The goal is to find the structure or distribution of the input data. The most common type of unsupervised algorithm is clustering where we try to group the training set according to similarities. For instance we can group customers by shopping habits.

**3.What is normalization and why do you perform? Explain with examples. What are the different techniques used? (2 points)**

Normalization is the process to convert columns in a dataset that were measured in different scales to a common scale usually between 0 and 1.

It is used because different machine learning algorithms may be sensitive to the scale of the features. For instance if we use the Euclidean distance in kNN algorithm, the feature that uses the bigger scale will have more weight on the decision to find the nearest neighbors if we don’t normalize data.

Lets take an example where we have two features to try to predict the damage of a building in an earthquake: one of them is magnitude of earthquake (Richter) and the other is the number of storeys in the building.

Let’s assume our training set has one point with a magnitude 2 and the number of storeys is 20 and another point with a magnitude of 7 and number of storeys is 25. If we are given a 3rd test point where the magnitude is 3.5 and the storeys are 24 and we calculate the Euclidean distance then the nearest point of the two mentioned will be the one with the magnitude 7 earthquake since there is only 1 floor difference and 3.5 magnitude difference compared to the other point with 1.5 magnitude and 4 floors of difference.

If we assume that the magnitude in the whole training set has a range from 1 to 8 and the number of storeys has a range from 1 to 50 and we normalize from 0 to 1 both features we will see that the magnitude difference is much more important than the floor difference in this case.

Some techniques used (from lecture2b) are:

* Min and max:

$$v' = \frac{v-min_A}{max_A-min_A}(newmax_A-newmin_A) +newmin_A$$

* Z-score:
$$v' = \frac{v-mean_A}{stdev_A}$$

* Decimal scaling
$$v' = \frac{v}{10^j}$$
Where $j$ is the smallest integer such that $Max(|v'|)<1$

**4.What does K stand in K-nearest neighbor algorithm? How do you determine the optimal value of K. Take Wisconsin cancer data (provided), determine the optimal
value of K. Plot misclassification error for test as well as training data. ( 4 points)**

The parameter *k* is the number of nearest neighbors to be found by the algorithm given an input test point. The majority class label will be assigned to the test point.

```{r data_setup, include=FALSE}
#source('KNN.r')
```

The parameter is typically chosen to be an odd number to avoid a tie. As a rule of thumb we can pick *k* to be the square root of the number of elements in the training set:

```{r k_size}
#k_thumb <- sqrt(nrow(wbcd_train))
#k_thumb
```

Trying different values of *k*:

```{r k_test}
#k_test <- seq(1,k_thumb*2,2)
#k_test
```
