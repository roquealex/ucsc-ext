---
title: "ASSIGNMENT 2"
author: "Roque Arcudia"
date: "February 26, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**1. What assumptions are made about the attributes in Naïve Bayes method of classification and why? What is Laplacian correction and why it is necessary?**

The assumptions in the naïve bayes are that the features in the data set are equally important and independent. This simplifies the calculation of the conditional probability $p(X|C_k)$ to be a simple multiplication:

$$p(X|C_k) = \prod_{i=1}^np(x_i|C_k)$$
Where $X$ is the input feature vector and $x_i$ is the each individual feature value.

This reduces the compute power needed to apply the model. It also reduces the size of the set required to train the model.

A flaw in naïve bayes happens when one of the factors in the products of probabilities is zero because no example had that attribute in the training set for a particular class converting the whole probability in zero. The zero probability will dominate the final probability. For that we apply Laplace smoothing. According to Wikipedia this is the relationship:

$$\hat{\theta_i}=\frac{x_i+\alpha}{N+\alpha d}$$
$$(i=1,...,d)$$
Where $\alpha>0$ is the smoothing parameter and as a result the estimate will create a value between the original probability $x_i/N$ and the uniform probability $1/d$

**2.How can a decision tree be converted into a rule set? Illustrate with an example. What are the advantages of the rule set representation over the decision tree representation?**


To convert a decision tree to a ruleset every leaf of the tree becomes a rule. To find the rule for the leaf we need to find the path from the root of the tree to the leaf and happy a logical "and" to all those conditions. An extra step would be to simplify the rule if there is redundancy.

For instance this will be a decision tree extracted from the iris dataset:


```{r dtree_example,echo=FALSE}
library(rpart)
library(rpart.plot)
data(iris)
set.seed(1)
dtree <- rpart(Species~., data=iris, method="class", parms=list(split="information"))
prp(dtree, type=3,fallen.leaves=TRUE,main="Iris Decision Tree")

```

And the rules:

* if(Petal.Length < 2.5) Species = setosa
* if(Petal.length >= 2.5 AND Petal.width < 1.8) Species = versicolor
* if(Petal.length >= 2.5 AND Petal.width >= 1.8) Species = virginica

The advantage of the rule representation is that it is easier to implement or use  as a report or in a database system.

**3.Take the iris data. Use Naïve Bayes algorithm to find the species with the following two observations with attributes
[4.5 3.0 5.6 2.1; 5.4 2.6 4.5 0.0] Don’t use the any R package. Show
various steps involved and do as explained in the class.**

Importing iris dataset and dplyr for better dataframe handling

```{r iris_imports,message=FALSE}
library(datasets)
library(dplyr)
data(iris)
```

Calculating the mean and the variance for each feature for each class.

```{r iris_summary}
iris.gp.by.Species <- iris %>% group_by(Species)

iris.mean.gp.by.Species <- iris.gp.by.Species %>% summarise_all(mean)
knitr::kable(iris.mean.gp.by.Species,caption = "Means by class")

iris.var.gp.by.Species <- iris.gp.by.Species %>% summarise_all(var)
knitr::kable(iris.var.gp.by.Species, caption = "Variances by class")
```

Calculate the probability of each class.

```{r iris_prob}
ntotal <- nrow(iris)
iris.Prob.gp.by.Species <- iris.gp.by.Species %>%
  summarise(Prob=n()/ntotal)
knitr::kable(iris.Prob.gp.by.Species, caption = "P(class)")
```

Preparing the input test vectors for classification.

```{r test_vectors}
feature_names <- names(iris)
feature_names <- feature_names[feature_names!="Species"]
test.data <- as.data.frame(
  matrix(c(4.5, 3.0, 5.6, 2.1, 5.4, 2.6, 4.5, 0.0),nrow=2,byrow=TRUE)
  )
names(test.data) <- feature_names
vector_names <- sapply(c(1:nrow(test.data)),function(x) {paste("Vector",x)})
row.names(test.data)<-vector_names
knitr::kable(test.data,caption = "Vectors")
```

Calculating the PDFs using a normal distribution using the mean and variance previously calculated. Creating my own function taking the input vector and a vector of mean and variance.

```{r test_pdf}
calculateNorm <- function(x,input.mean,input.var) {
  return(exp((-(x-input.mean)^2)/(2*input.var))/sqrt(2*pi*input.var))
}

calculatePdfs <- function(input.data) {
  species_names <- iris.Prob.gp.by.Species$Species
  pdfs <- sapply(iris.Prob.gp.by.Species$Species, function(x) {
    this.mean <- iris.mean.gp.by.Species[
      iris.mean.gp.by.Species$Species==x,
      feature_names
      ]
    this.var <- iris.var.gp.by.Species[
        iris.var.gp.by.Species$Species==x,
        feature_names
        ]
    # Future summarise doesn't work if it is not numeric
    #pdf <- calculateNorm((input.data),(this.mean),(this.var))
    pdf <- calculateNorm(
      as.numeric(input.data),
      input.mean=as.numeric(this.mean),
      input.var=as.numeric(this.var)
      )
    return(pdf)
  })
  rownames(pdfs) <- feature_names
  colnames(pdfs) <- species_names
  return(pdfs)
}

pdfs.per.vector <- list()

for(i in 1:nrow(test.data)){
  pdfs <- calculatePdfs(test.data[i,])
  pdfs.per.vector[[i]] <- pdfs
}

pdfs.per.vector
```

The table will be read the value of the PDF of “row name” given “column name” for instance p(Sepal.Length=`r test.data[1,1]`|setosa)=`r pdfs.per.vector[[1]][1,1]`

Multiplying the results of all the PDF values by the class probability too and picking up the class of the highest number. For each class (setosa, versicolor and virginica) we calculate:

$$P(class)p(Sepal.Length|class)p(Sepal.Width|class)p(Petal.Length|class)p(Petal.Width|class)$$

```{r iris_class}
posteriors.per.vector<- t(sapply(pdfs.per.vector, function(input.pdfs) {
  probs <- rbind(input.pdfs,iris.Prob.gp.by.Species$Prob)
  posteriors <- as.data.frame(probs) %>% summarise_all(prod)
  winner <- names(posteriors)[which.max(posteriors)]
  return(posteriors %>% mutate(winner=winner))
}))

row.names(posteriors.per.vector)<-vector_names
knitr::kable(posteriors.per.vector,caption = "Classification")

```

The calculation of the “evidence” is omitted since will be a constant and won’t affect the result of the comparison.

```{r iris_extraction, include=FALSE}
# Deprecated code, new une uses dplyr and more compact
library(datasets)
data(iris)

ntotal <- nrow(iris)
setosa_total <- sum(iris$Species=="setosa")
versicolor_total <- sum(iris$Species=="versicolor")
virginica_total <- sum(iris$Species=="virginica")

ntotal
setosa_total
versicolor_total
virginica_total

P_setosa<- setosa_total/ntotal
P_versicolor<- versicolor_total/ntotal
P_virginica<- virginica_total/ntotal

P_setosa
P_versicolor
P_virginica

feature_names <- names(iris)
feature_names <- feature_names[feature_names!="Species"]
feature_names
#num_cols <- names(iris)!="Species"
iris_setosa <- iris[iris$Species=="setosa",feature_names]
iris_versicolor <- iris[iris$Species=="versicolor",feature_names]
iris_virginica <- iris[iris$Species=="virginica",feature_names]
  
iris_setosa
iris_versicolor
iris_virginica
summary(iris_setosa)
summary(iris_versicolor)
summary(iris_virginica)

#iris_setosa_mean <-colMeans(iris_setosa)
#iris_versicolor_mean <- colMeans(iris_versicolor)
#iris_virginica_mean <- colMeans(iris_virginica)

iris_setosa_mean <-sapply(iris_setosa,mean)
iris_versicolor_mean <- sapply(iris_versicolor,mean)
iris_virginica_mean <- sapply(iris_virginica,mean)

iris_setosa_mean
iris_versicolor_mean
iris_virginica_mean

iris_setosa_var <-sapply(iris_setosa,var)
iris_versicolor_var <- sapply(iris_versicolor,var)
iris_virginica_var <- sapply(iris_virginica,var)

iris_setosa_var
iris_versicolor_var
iris_virginica_var

#[4.5 3.0 5.6 2.1; 5.4 2.6 4.5 0.0]

my_norm <- function(x,meanx,varx) {
	return(exp((-(x-meanx)^2)/(2*varx))/sqrt(2*pi*varx))
}
#Sepal.Length
p_Sepal.Length_setosa <- my_norm(4.5, iris_setosa_mean['Sepal.Length'],iris_setosa_var['Sepal.Length'])
p_Sepal.Length_versicolor <- my_norm(4.5, iris_versicolor_mean['Sepal.Length'],iris_versicolor_var['Sepal.Length'])
p_Sepal.Length_virginica <- my_norm(4.5, iris_virginica_mean['Sepal.Length'],iris_virginica_var['Sepal.Length'])

p_Sepal.Length_setosa
p_Sepal.Length_versicolor
p_Sepal.Length_virginica

#Sepal.Width
p_Sepal.Width_setosa <- my_norm(3.0, iris_setosa_mean['Sepal.Width'],iris_setosa_var['Sepal.Width'])
p_Sepal.Width_versicolor <- my_norm(3.0, iris_versicolor_mean['Sepal.Width'],iris_versicolor_var['Sepal.Width'])
p_Sepal.Width_virginica <- my_norm(3.0, iris_virginica_mean['Sepal.Width'],iris_virginica_var['Sepal.Width'])

p_Sepal.Width_setosa
p_Sepal.Width_versicolor
p_Sepal.Width_virginica




#Petal.Length
p_Petal.Length_setosa <- my_norm(5.6, iris_setosa_mean['Petal.Length'],iris_setosa_var['Petal.Length'])
p_Petal.Length_versicolor <- my_norm(5.6, iris_versicolor_mean['Petal.Length'],iris_versicolor_var['Petal.Length'])
p_Petal.Length_virginica <- my_norm(5.6, iris_virginica_mean['Petal.Length'],iris_virginica_var['Petal.Length'])

p_Petal.Length_setosa
p_Petal.Length_versicolor
p_Petal.Length_virginica

#Petal.Width
p_Petal.Width_setosa <- my_norm(2.1, iris_setosa_mean['Petal.Width'],iris_setosa_var['Petal.Width'])
p_Petal.Width_versicolor <- my_norm(2.1, iris_versicolor_mean['Petal.Width'],iris_versicolor_var['Petal.Width'])
p_Petal.Width_virginica <- my_norm(2.1, iris_virginica_mean['Petal.Width'],iris_virginica_var['Petal.Width'])

p_Petal.Width_setosa
p_Petal.Width_versicolor
p_Petal.Width_virginica

#Goldens:
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##        5.006        3.428        1.462        0.246
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##        5.936        2.770        4.260        1.326
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##        6.588        2.974        5.552        2.026

## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##   0.12424898   0.14368980   0.03015918   0.01110612
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##   0.26643265   0.09846939   0.22081633   0.03910612
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##   0.40434286 0.10400408 0.30458776 0.07543265


posterior_setosa <- P_setosa * p_Sepal.Length_setosa * p_Sepal.Width_setosa * p_Petal.Length_setosa * p_Petal.Width_setosa
posterior_versicolor <- P_versicolor * p_Sepal.Length_versicolor * p_Sepal.Width_versicolor * p_Petal.Length_versicolor * p_Petal.Width_versicolor
posterior_virginica <- P_virginica * p_Sepal.Length_virginica * p_Sepal.Width_virginica * p_Petal.Length_virginica * p_Petal.Width_virginica

posterior_setosa
posterior_versicolor
posterior_virginica

## 2.093198e-191

```



